{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a9659e-d5b2-4969-bda9-5424ea0eefd6",
   "metadata": {},
   "source": [
    "## [요구사항 1]\ttitanic 딥러닝 모델 기본 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7154d02b-b8b6-42de-8436-ee60a96312d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTE] WandB has been manually set to: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: le993939 (le993939-korea-university-of-technology-and-education) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\le993\\git\\link_dl\\_04_your_code\\wandb\\run-20251013_143143-oycax1cd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/oycax1cd' target=\"_blank\">2025-10-13_14-31-42</a></strong> to <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/oycax1cd' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/oycax1cd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(wandb=True, batch_size=64, epochs=500)\n",
      "{'epochs': 500, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [32, 16]}\n",
      "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n",
      "Train/Validation Split: 712 / 179\n",
      "--------------------------------------------------\n",
      "MyModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "##################################################\n",
      "Epoch 100, Train Loss 0.6089, Val Loss 0.6021, Train Acc 0.6896, Val Acc 0.7095\n",
      "Epoch 200, Train Loss 0.5857, Val Loss 0.5921, Train Acc 0.7107, Val Acc 0.7039\n",
      "Epoch 300, Train Loss 0.5807, Val Loss 0.5903, Train Acc 0.7149, Val Acc 0.7151\n",
      "Epoch 400, Train Loss 0.5917, Val Loss 0.5876, Train Acc 0.6966, Val Acc 0.7151\n",
      "Epoch 500, Train Loss 0.5684, Val Loss 0.5749, Train Acc 0.7037, Val Acc 0.7039\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>Training accuracy</td><td>▁▁▁▂▂▁▁▃▂▂▅▅▆▆█▇▇▇▆▆▆▇▆█▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>Training loss</td><td>█▃▃▂▂▂▂▂▃▃▂▃▂▂▂▃▁▂▁▂▁▁▂▁▁▁▁▁▂▂▁▁▁▁▁▂▂▁▂▂</td></tr><tr><td>Validation accuracy</td><td>▄▄▅▄▄▄▅▄▄▂▁▁▅█▅▅▄▅▄▅▅▅▄▅▅▅▅▅▇▅▇▂▁▄▂▅▁▂▄▂</td></tr><tr><td>Validation loss</td><td>█▅▇▆▆▅▅▅▅▅▅▅▄▅▅▄▃▄▄▄▄▃▄▂▅▄▃▄▃▃▁▄▂▄▁▄▄▄▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>500</td></tr><tr><td>Training accuracy</td><td>0.70365</td></tr><tr><td>Training loss</td><td>0.56842</td></tr><tr><td>Validation accuracy</td><td>0.70391</td></tr><tr><td>Validation loss</td><td>0.57495</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-10-13_14-31-42</strong> at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/oycax1cd' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/oycax1cd</a><br> View project at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251013_143143-oycax1cd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 출력 옵션 설정 (데이터 전처리 디버깅용)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "# --- 1. Titanic Dataset 클래스 정의 ---\n",
    "class TitanicDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    # PyTorch 텐서로 변환\n",
    "    self.X = torch.FloatTensor(X)\n",
    "    # 분류 문제의 타겟은 일반적으로 LongTensor 사용\n",
    "    self.y = torch.LongTensor(y) \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    target = self.y[idx]\n",
    "    # 이전 예제와 동일하게 딕셔너리 키를 'input'과 'target'으로 유지\n",
    "    return {'input': feature, 'target': target}\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
    "\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "  def __init__(self, X):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    return {'input': feature}\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}\"\n",
    "\n",
    "\n",
    "# --- 2. 데이터 전처리 보조 함수 (FutureWarning 해결) ---\n",
    "\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    all_df = all_df.drop(columns=[\"Fare_mean\"])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"title\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"title\"] = name_df[\"title\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    title_age_mean = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index()\n",
    "    title_age_mean.columns = [\"title\", \"title_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, title_age_mean, on=\"title\", how=\"left\")\n",
    "    # 결측치 대체 전에 존재하지 않는 title의 중앙값(NaN)은 0으로 임시 대체 (Merge에서 발생 가능)\n",
    "    all_df[\"title_age_mean\"] = all_df[\"title_age_mean\"].fillna(0) \n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"title_age_mean\"]\n",
    "    all_df = all_df.drop([\"title_age_mean\"], axis=1)\n",
    "    # Age가 여전히 NaN인 경우(매우 희귀한 title의 Age가 모두 NaN인 경우), 전체 평균으로 최종 대체\n",
    "    all_df[\"Age\"] = all_df[\"Age\"].fillna(all_df[\"Age\"].mean())\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    # 수정: inplace=True 대신 직접 할당을 사용하여 FutureWarning 해결\n",
    "    all_df[\"alone\"] = all_df[\"alone\"].fillna(0)\n",
    "    \n",
    "    # 학습에 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # title 값 개수 줄이기\n",
    "    main_titles = [\"Mr\", \"Miss\", \"Mrs\", \"Master\"]\n",
    "    all_df.loc[\n",
    "    ~all_df[\"title\"].isin(main_titles),\n",
    "    \"title\"\n",
    "    ] = \"other\"\n",
    "    # 수정: inplace=True 대신 직접 할당을 사용하여 FutureWarning 해결\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 카테고리 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        # 모든 데이터(train + test)를 이용해 fit 후 transform\n",
    "        all_df[category_feature] = le.fit_transform(all_df[category_feature])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "# --- 3. 데이터 로드 및 DataLoader 생성 함수 (California Housing 스타일로 통합) ---\n",
    "def get_data():\n",
    "    # 데이터 파일 경로 설정 (실행 환경에 맞게 조정 필요)\n",
    "    # 가정: train.csv와 test.csv가 현재 작업 디렉토리에 있음\n",
    "    train_data_path = os.path.join(os.getcwd(), \"train.csv\")\n",
    "    test_data_path = os.path.join(os.getcwd(), \"test.csv\")\n",
    "\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"에러: 'train.csv' 또는 'test.csv' 파일을 찾을 수 없습니다. 경로를 확인해 주세요.\")\n",
    "        raise\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    # 전처리 단계 순차적 실행\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "    \n",
    "    # 학습/검증 데이터 (Survived가 null이 아닌 행) 분리\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    print(dataset)\n",
    "    \n",
    "    # 학습 80%, 검증 20% 분할\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    validation_size = len(dataset) - train_size\n",
    "    train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "    \n",
    "    print(f\"Train/Validation Split: {len(train_dataset)} / {len(validation_dataset)}\")\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    # 검증은 전체를 하나의 배치로 처리\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=validation_size)\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "\n",
    "# --- 4. 모델 정의 (이전과 동일) ---\n",
    "class MyModel(nn.Module):\n",
    "  N_INPUT = 10 \n",
    "  N_OUTPUT = 1 \n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(self.N_INPUT, wandb.config.n_hidden_unit_list[0]),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[1], self.N_OUTPUT),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "  my_model = MyModel()\n",
    "  optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "  return my_model, optimizer\n",
    "\n",
    "\n",
    "# --- 5. 학습 루프 (이전과 동일) ---\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "  n_epochs = wandb.config.epochs\n",
    "  loss_fn = nn.BCEWithLogitsLoss()\n",
    "  next_print_epoch = 100\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    num_trains = 0\n",
    "    num_correct_train = 0\n",
    "    num_total_train = 0\n",
    "\n",
    "    for train_batch in train_data_loader:\n",
    "      input = train_batch['input']\n",
    "      target = train_batch['target'].float().unsqueeze(1) \n",
    "      \n",
    "      output_train = model(input)\n",
    "      loss = loss_fn(output_train, target)\n",
    "      loss_train += loss.item()\n",
    "      num_trains += 1\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      prediction_train = (torch.sigmoid(output_train) > 0.5).long()\n",
    "      num_correct_train += (prediction_train == target.long()).sum().item()\n",
    "      num_total_train += target.size(0)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss_validation = 0.0\n",
    "    num_validations = 0\n",
    "    num_correct_validation = 0\n",
    "    num_total_validation = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in validation_data_loader:\n",
    "        input = validation_batch['input']\n",
    "        target = validation_batch['target'].float().unsqueeze(1)\n",
    "        \n",
    "        output_validation = model(input)\n",
    "        loss = loss_fn(output_validation, target)\n",
    "        loss_validation += loss.item()\n",
    "        num_validations += 1\n",
    "\n",
    "        prediction_validation = (torch.sigmoid(output_validation) > 0.5).long()\n",
    "        num_correct_validation += (prediction_validation == target.long()).sum().item()\n",
    "        num_total_validation += target.size(0)\n",
    "\n",
    "    # 지표 계산\n",
    "    train_loss_avg = loss_train / num_trains\n",
    "    val_loss_avg = loss_validation / num_validations\n",
    "    train_accuracy = num_correct_train / num_total_train\n",
    "    val_accuracy = num_correct_validation / num_total_validation\n",
    "\n",
    "    # wandb 로깅\n",
    "    wandb.log({\n",
    "      \"Epoch\": epoch,\n",
    "      \"Training loss\": train_loss_avg,\n",
    "      \"Validation loss\": val_loss_avg,\n",
    "      \"Training accuracy\": train_accuracy,\n",
    "      \"Validation accuracy\": val_accuracy,\n",
    "    })\n",
    "\n",
    "    if epoch >= next_print_epoch:\n",
    "      print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Train Loss {train_loss_avg:.4f}, \"\n",
    "        f\"Val Loss {val_loss_avg:.4f}, \"\n",
    "        f\"Train Acc {train_accuracy:.4f}, \"\n",
    "        f\"Val Acc {val_accuracy:.4f}\"\n",
    "      )\n",
    "      next_print_epoch += 100\n",
    "\n",
    "\n",
    "# --- 6. 메인 실행 함수 (이전과 동일) ---\n",
    "def main(args):\n",
    "  current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "  config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'learning_rate': 1e-3, # 초기 학습률\n",
    "    'n_hidden_unit_list': [32, 16], # 은닉층 뉴런 수 조정\n",
    "  }\n",
    "\n",
    "  wandb.init(\n",
    "    mode=\"online\" if args.wandb else \"disabled\",\n",
    "    project=\"titanic_survival_prediction\", # 프로젝트 이름 변경\n",
    "    notes=\"Titanic Classification with Preprocessing\",\n",
    "    tags=[\"titanic\", \"classification\"],\n",
    "    name=current_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  print(args)\n",
    "  print(wandb.config)\n",
    "\n",
    "  train_data_loader, validation_data_loader = get_data()\n",
    "\n",
    "  linear_model, optimizer = get_model_and_optimizer()\n",
    "  print(\"-\" * 50)\n",
    "  print(linear_model)\n",
    "  print(\"#\" * 50)\n",
    "\n",
    "  training_loop(\n",
    "    model=linear_model,\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader\n",
    "  )\n",
    "  wandb.finish()\n",
    "\n",
    "\n",
    "# --- 7. 명령줄 인수 처리 (WandB 활성화 수정) ---\n",
    "if __name__ == \"__main__\":\n",
    "  # Python 스크립트 실행 경로를 현재 경로로 임시 설정 (데이터 로드를 위해)\n",
    "  # os.chdir(os.path.dirname(os.path.abspath(__file__))) \n",
    "  \n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  # argparse 기본값은 False지만, Jupyter에서는 명시적 설정이 필요\n",
    "  parser.add_argument(\n",
    "    \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"True or False\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-b\", \"--batch_size\", type=int, default=64, help=\"Batch size (int, default: 64)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-e\", \"--epochs\", type=int, default=500, help=\"Number of training epochs (int, default: 500)\"\n",
    "  )\n",
    "\n",
    "  # 1. argparse의 기본 동작을 먼저 수행 (args.wandb는 default=False)\n",
    "  args = parser.parse_args([])\n",
    "\n",
    "  # 2. WandB를 활성화하기 위해 args 객체의 wandb 속성을 명시적으로 True로 덮어씌움\n",
    "  # Jupyter Notebook에서 가장 확실하게 WandB를 켜는 방법\n",
    "  args.wandb = True\n",
    "\n",
    "  print(f\"[NOTE] WandB has been manually set to: {args.wandb}\")\n",
    "\n",
    "  main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d410c3c8-f675-4dc7-8090-ff8937a2c7a1",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- all_df를 통해 학습 데이터와 테스트 데이터를 합친 통합 데이터프레임를 입력받을 수 있음\n",
    "- reset_index() : groupby 결과를 일반 DataFrame 형태로 되돌림\n",
    "- .loc : Pandas의 인덱서 중 하나로 데이터프레임의 특정 위치에 접근핧 수 있다. [행 선택 조건, 열 선택]의 형식을 가진다.\n",
    "- .round() : 소수점이 생기면 반올림해서 정수 형태로 만듬\n",
    "- ~ : 논리 부정(NOT) 연산자\n",
    "- LabelEncoder : sklearn.preprocessing의 클래스로 문자열로 되어 있는 범주(label)를 숫자(label index)로 바꿔주는 인코더\n",
    "- .fit() : 데이터 안에 있는 고유한 값(unique categories) 을 학습\n",
    "- .transform() : 각 값을 고유한 정수로 바꿈\n",
    "- .fit_transform() : 둘을 한 번에 수행하는 메서드\n",
    "- pd.concat()을 통해 학습 데이터와 테스트 데이터를 합침 => 이는 전처리를 train/test 모두 같은 기준으로 일관되게 수행하기 위함임\n",
    "- shuffle=True : 매 epoch마다 데이터 순서를 섞어 학습 안정성 향상\n",
    "- nn.Sequential : 여러 개의 레이어(층)를 순차적으로 연결하여 신경망 구조를 정의\n",
    "- optim.SGD : 확률적 경사 하강법 최적화 도구를 정의\n",
    "- lr=wandb.config.learning_rate : 학습률 설정\n",
    "- nn.BCEWithLogitsLoss() : 손실 함수. sigmoid + binary cross entropy를 한 번에 계산.\n",
    "- torch.sigmoid(output_train) > 0.5를 통해 로짓 출력에 시그모이드를 적용하여 확률을 구하고, 0.5를 기준으로 이진 예측을 수행하여 정확도 측정\n",
    "- with torch.no_grad()를 통해 검증 시에는 기울기 계산을 비활성화 → 메모리와 시간 절약\n",
    "- wandb.log : 각 epoch의 주요 지표를 wandb 대시보드에 기록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f379e3a-ca33-4bc4-bf89-b132fc58194e",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- get_preprocessed_dataset_1(all_df) 함수를 통해 데이터가 깨끗해지고, 머신러닝 모델 학습 시 결측치로 인한 오류를 방지할 수 있음\n",
    "- get_preprocessed_dataset_2 함수를 통해 Name 컬럼에서 호칭(Title) 정보를 추출하여 나이(Age)의 결측치를 채울 수 있음\n",
    "- get_preprocessed_dataset_3 함수에서는 title별로 Age의 중앙값(median) 을 구하는데 이는 평균(mean)보다 극단값(이상치) 의 영향을 덜 받기 때문이다.\n",
    "- get_preprocessed_dataset_4 함수는 직계 가족의 총 인원수을 계산하는데, 이는 가족 규모가 생존 확률과 강한 상관관계가 있는 것으로 알려져 있기 때문이다.\n",
    "- get_preprocessed_dataset_5 함수는 데이터 불균형을 줄이고, 모델의 일반화 성능을 높이기 위해 나머지는 모두 \"other\"로 묶는다. 또한 Embarked의 결측치를 \"missing\"으로 채워 카테고리형 변수를 단순하고 안정적으로 정리한다.\n",
    "- get_preprocessed_dataset_6 함수는 신경망(PyTorch 모델)은 문자열 데이터를 직접 처리할 수 없으므로, 학습이 가능하도록 모든 데이터를 수치화하는 Label Encoding을 하지만, LabelEncoder가 정수를 할당하는 과정에서 값 사이에 순서(Ordinality)가 있는 것처럼 모델에게 잘못된 정보를 전달할 수 있다는 문제점이 존재한다. 따라서  순서가 없는 범주형 데이터(Nominal Data)의 경우, LabelEncoder보다 원-핫 인코딩(One-Hot Encoding)을 사용하는 것이 더 안전하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231af362-2156-4c3f-b322-a6f8a53246bc",
   "metadata": {},
   "source": [
    "## [요구사항 2]\tActivation\tFunction\t과\tBatch\tSize\t변경\t및\t선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa261edd-a6ff-4dd3-adee-150c317229f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTE] WandB manually set to: True\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\le993\\git\\link_dl\\_04_your_code\\wandb\\run-20251017_143101-ayz3hu3f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/ayz3hu3f' target=\"_blank\">2025-10-17_14-31-01</a></strong> to <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/ayz3hu3f' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/ayz3hu3f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Activation: relu]\n",
      "\n",
      "[Activation: elu]\n",
      "\n",
      "[Activation: leaky_relu]\n",
      "\n",
      "[Activation: sigmoid]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▄▄▄▅▆▆▇█▂▂▃▃▄▆▆▇▇██▁▂▃▃▄▅▆▆▇█▁▃▃▄▅▆▆▆█</td></tr><tr><td>Training accuracy</td><td>▅▅▅▆▅▅▇▇▇▇▅█▇███▆▆▇▆▆▆▇▆▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training loss</td><td>▆▅▅▃▄▅▃▃▄▃▅▂▃▂▂▂▃▂▂▁▅▄▄▄▃▄▃▃▃▃▃▂▃█▇▇▇███</td></tr><tr><td>Validation accuracy</td><td>▇▇▇▇▇▆▇▆▇▆██▇▆█▆▇▅▇▇▇▇▇▇▆▆▆▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>▂▂▂▂▃▂▂▂▂▂▂▁▁▁▃▃▃▂▂▂▂▂▂▂▂█▇▇▇▇▇▇▇▇▇▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>500</td></tr><tr><td>Training accuracy</td><td>0.60674</td></tr><tr><td>Training loss</td><td>0.66639</td></tr><tr><td>Validation accuracy</td><td>0.65363</td></tr><tr><td>Validation loss</td><td>0.64708</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-10-17_14-31-01</strong> at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/ayz3hu3f' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/ayz3hu3f</a><br> View project at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a><br>Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_143101-ayz3hu3f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go \n",
    "\n",
    "# 출력 옵션 설정\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "# --- 1. Titanic Dataset 클래스 정의 ---\n",
    "class TitanicDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "    self.y = torch.LongTensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}, Target Shape: {self.y.shape}\"\n",
    "\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "  def __init__(self, X):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {'input': self.X[idx]}\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"Data Size: {len(self.X)}, Input Shape: {self.X.shape}\"\n",
    "\n",
    "\n",
    "# --- 2. 전처리 함수들 ---\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    all_df = all_df.drop(columns=[\"Fare_mean\"])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"title\", \"name\"]\n",
    "    name_df = name_df.apply(lambda col: col.str.strip())\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    title_age_mean = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index()\n",
    "    title_age_mean.columns = [\"title\", \"title_age_mean\"]\n",
    "    all_df = pd.merge(all_df, title_age_mean, on=\"title\", how=\"left\")\n",
    "    all_df[\"title_age_mean\"] = all_df[\"title_age_mean\"].fillna(0)\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"title_age_mean\"]\n",
    "    all_df = all_df.drop([\"title_age_mean\"], axis=1)\n",
    "    all_df[\"Age\"] = all_df[\"Age\"].fillna(all_df[\"Age\"].mean())\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"] = all_df[\"alone\"].fillna(0)\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    main_titles = [\"Mr\", \"Miss\", \"Mrs\", \"Master\"]\n",
    "    all_df.loc[~all_df[\"title\"].isin(main_titles), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    for feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        all_df[feature] = le.fit_transform(all_df[feature])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "# --- 3. 데이터 로드 ---\n",
    "def get_data():\n",
    "    train_data_path = os.path.join(os.getcwd(), \"train.csv\")\n",
    "    test_data_path = os.path.join(os.getcwd(), \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# --- 4. 모델 정의 ---\n",
    "def get_activation(name):\n",
    "    name = name.lower()\n",
    "    if name == \"relu\": return nn.ReLU()\n",
    "    if name == \"elu\": return nn.ELU()\n",
    "    if name in (\"leaky_relu\", \"leaky-relu\", \"lrelu\"): return nn.LeakyReLU(0.01)\n",
    "    if name == \"sigmoid\": return nn.Sigmoid()\n",
    "    raise ValueError(f\"Unknown activation: {name}\")\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  N_INPUT = 10\n",
    "  N_OUTPUT = 1\n",
    "  def __init__(self, activation_name=\"relu\"):\n",
    "    super().__init__()\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(self.N_INPUT, wandb.config.n_hidden_unit_list[0]),\n",
    "        get_activation(activation_name),\n",
    "        nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "        get_activation(activation_name),\n",
    "        nn.Linear(wandb.config.n_hidden_unit_list[1], self.N_OUTPUT),\n",
    "    )\n",
    "  def forward(self, x): return self.model(x)\n",
    "\n",
    "\n",
    "def get_model_and_optimizer(activation_name):\n",
    "    model = MyModel(activation_name)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "# --- 5. 학습 루프 ---\n",
    "def training_loop(model, optimizer, train_loader, val_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    best_val_acc = -1.0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    train_hist, val_hist = [], []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        loss_train, correct_train, total_train = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch['input'], batch['target'].float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            pred = (torch.sigmoid(output) > 0.5).long()\n",
    "            correct_train += (pred == y.long()).sum().item()\n",
    "            total_train += y.size(0)\n",
    "        train_loss_avg = loss_train / len(train_loader)\n",
    "        train_hist.append(train_loss_avg)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x, y = batch['input'], batch['target'].float().unsqueeze(1)\n",
    "                output = model(x)\n",
    "                loss = loss_fn(output, y)\n",
    "                val_loss += loss.item()\n",
    "                pred = (torch.sigmoid(output) > 0.5).long()\n",
    "                correct_val += (pred == y.long()).sum().item()\n",
    "                total_val += y.size(0)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_hist.append(val_loss_avg)\n",
    "\n",
    "        train_acc = correct_train / total_train\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        # best 추적\n",
    "        if val_acc > best_val_acc: best_val_acc = val_acc\n",
    "        if val_loss_avg < best_val_loss: best_val_loss = val_loss_avg\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training loss\": train_loss_avg,\n",
    "            \"Validation loss\": val_loss_avg,\n",
    "            \"Training accuracy\": train_acc,\n",
    "            \"Validation accuracy\": val_acc,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"train_loss_history\": train_hist,\n",
    "        \"val_loss_history\": val_hist,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 6. 메인 ---\n",
    "def main(args):\n",
    "    current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': 1e-3,\n",
    "        'n_hidden_unit_list': [32, 16],\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"titanic_survival_prediction\",\n",
    "        notes=\"Activation function comparison (Plotly + backup line_series)\",\n",
    "        tags=[\"titanic\", \"activation-compare\", \"plotly\"],\n",
    "        name=current_time_str,\n",
    "        config=config,\n",
    "        settings=wandb.Settings(_disable_stats=True),  # system metric 비활성화\n",
    "    )\n",
    "\n",
    "    train_loader, val_loader = get_data()\n",
    "    activations = [\"relu\", \"elu\", \"leaky_relu\", \"sigmoid\"]\n",
    "\n",
    "    train_hist_by_act, val_hist_by_act = {}, {}\n",
    "    for act in activations:\n",
    "        wandb.config.update({\"activation\": act}, allow_val_change=True)\n",
    "        model, opt = get_model_and_optimizer(act)\n",
    "        print(f\"\\n[Activation: {act}]\")\n",
    "        metrics = training_loop(model, opt, train_loader, val_loader)\n",
    "        train_hist_by_act[act] = metrics[\"train_loss_history\"]\n",
    "        val_hist_by_act[act] = metrics[\"val_loss_history\"]\n",
    "\n",
    "    # --- Plotly 그래프 생성 (색상 구분 확실) ---\n",
    "    epochs = list(range(1, wandb.config.epochs + 1))\n",
    "    color_map = {\n",
    "        \"relu\": \"#1f77b4\",        # 파랑\n",
    "        \"elu\": \"#ff7f0e\",         # 주황\n",
    "        \"leaky_relu\": \"#2ca02c\",  # 초록\n",
    "        \"sigmoid\": \"#d62728\",     # 빨강\n",
    "    }\n",
    "\n",
    "    # Validation Loss Plot (Media 탭)\n",
    "    fig_val = go.Figure()\n",
    "    for act in activations:\n",
    "        fig_val.add_trace(go.Scatter(\n",
    "            x=epochs, y=val_hist_by_act[act],\n",
    "            mode=\"lines\",\n",
    "            name=act,\n",
    "            line=dict(color=color_map[act], width=2)\n",
    "        ))\n",
    "    fig_val.update_layout(\n",
    "        title=\"Validation Loss by Activation\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        legend_title=\"Activation\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    # Training Loss Plot (Media 탭)\n",
    "    fig_tr = go.Figure()\n",
    "    for act in activations:\n",
    "        fig_tr.add_trace(go.Scatter(\n",
    "            x=epochs, y=train_hist_by_act[act],\n",
    "            mode=\"lines\",\n",
    "            name=act,\n",
    "            line=dict(color=color_map[act], width=2)\n",
    "        ))\n",
    "    fig_tr.update_layout(\n",
    "        title=\"Training Loss by Activation\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        legend_title=\"Activation\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    # Plotly → Media 탭에 보이도록 래핑\n",
    "    wandb.log({\n",
    "        \"val_loss_compare_plotly\": wandb.Plotly(fig_val),\n",
    "        \"train_loss_compare_plotly\": wandb.Plotly(fig_tr),\n",
    "    }, commit=True)\n",
    "\n",
    "    # Charts 탭에도 보이도록 line_series 함께 로그\n",
    "    val_plot_backup = wandb.plot.line_series(\n",
    "        xs=epochs,\n",
    "        ys=[val_hist_by_act[a] for a in activations],\n",
    "        keys=activations,\n",
    "        title=\"Validation Loss by Activation (backup)\",\n",
    "        xname=\"Epoch\"\n",
    "    )\n",
    "    train_plot_backup = wandb.plot.line_series(\n",
    "        xs=epochs,\n",
    "        ys=[train_hist_by_act[a] for a in activations],\n",
    "        keys=activations,\n",
    "        title=\"Training Loss by Activation (backup)\",\n",
    "        xname=\"Epoch\"\n",
    "    )\n",
    "    wandb.log({\n",
    "        \"val_loss_compare_backup\": val_plot_backup,\n",
    "        \"train_loss_compare_backup\": train_plot_backup,\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# --- 7. 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", action=argparse.BooleanOptionalAction, default=False)\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=500)\n",
    "    args = parser.parse_args([])\n",
    "    args.wandb = True  # Jupyter에서 강제 on\n",
    "    print(f\"[NOTE] WandB manually set to: {args.wandb}\")\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f2187-c609-4f53-93b6-2d3a718b55da",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- get_activation 함수를 만들어 각각 ELU, ReLU, Leaky ReLU, Sigmoid에 해당하는 PyTorch 활성화 함수 모듈을 반환\n",
    "- activation_name=\"relu\" : MyModel 클래스에서 인수로 활성화 함수 이름을 받게 하기 위해 추가\n",
    "- get_activation(activation_name) : 동적으로 활성화 함수 적용\n",
    "- get_model_and_optimizer(activation_name) 함수 : activation_name으로 활성화 함수 이름 받기\n",
    "- train_hist_by_act[act] = metrics[\"train_loss_history\"] / val_hist_by_act[act] = metrics[\"val_loss_history\"]\n",
    "  : 각 모델의 학습 완료 후 손실 이력 저장\n",
    "- wandb.Plotly(figure) : 생성된 Plotly 그래프를 WandB 대시보드의 Media 탭에 로깅\n",
    "- wandb.plot.line_series : W&B의 기본 차트용(line plot) 객체를 생성하는 함수\n",
    "- ys=[val_hist_by_act[a] for a in activations] : 각 활성화 함수의 loss 변화 리스트를 y축 값에 넣기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6044e-4f4f-48e0-aeab-d1ba07b6c227",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- 한 차트 안에 ELU, ReLU, Leaky ReLU, Sigmoid를 비교하려니 선의 색이 같아 비교하기 어려운 문제가 발생했다. 이를 해결하기 위해 Plotly를 사용했다.\n",
    "- notebook 모드에서는 Plotly 그래프가 보이지 않는 환경이므로 Charts 탭에서 볼 수 있도록 백업 시각화를 추가해야 된다는 점\n",
    "- 현재 차트 안에서 ELU가 가장 빠르게 수렴하는 그래프를 보여주고 있으므로 더\t나은 성능을 산출하는\tActivation Function은 ELU이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b685c2d9-a50f-41fa-a92a-2d8696fe1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTE] WandB manually set to: True\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\le993\\git\\link_dl\\_04_your_code\\wandb\\run-20251017_173910-f21gmjzy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/f21gmjzy' target=\"_blank\">2025-10-17_17-39-10</a></strong> to <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/f21gmjzy' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/f21gmjzy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Batch Size: 16 | Activation: ELU]\n",
      "\n",
      "[Batch Size: 32 | Activation: ELU]\n",
      "\n",
      "[Batch Size: 64 | Activation: ELU]\n",
      "\n",
      "[Batch Size: 128 | Activation: ELU]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▄▄▆▆▆██▂▄▅▅▅▆▆▆▆▇▁▂▃▃▄▅▅▆▆█▁▂▂▂▃▄▄▅▅▇</td></tr><tr><td>Training accuracy</td><td>▁▁▁▂▃▆▇▇██▄▆▆▅▆▅▅▆▆█▂▅▅▆▆▆▆▆▂▃▃▃▃▃▃▄▄▄▅▄</td></tr><tr><td>Training loss</td><td>▇▇▇▆▆▆▆▆▅▅▂▂▁▇▇▆▆▆▅▄▄▇█▆▇▆▆▆▅▅▆▅▆▅▅▅▄▅▅▅</td></tr><tr><td>Validation accuracy</td><td>▂▄▄▅▅▅▅▅▆▆▆▆▆▅▇▇▇▇▇█▇█▆▅▅▅▅▅▅▅▂▂▁▁▁▂▂▂▃▃</td></tr><tr><td>Validation loss</td><td>▅▄▄▅▄▄▄▂▁▁▆▅▃▃▂▁▂▂▁▂▄▄▄▄▄▃▄▄▃▄▇██████▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>500</td></tr><tr><td>Training accuracy</td><td>0.71348</td></tr><tr><td>Training loss</td><td>0.56638</td></tr><tr><td>Validation accuracy</td><td>0.67598</td></tr><tr><td>Validation loss</td><td>0.62504</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-10-17_17-39-10</strong> at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/f21gmjzy' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/f21gmjzy</a><br> View project at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a><br>Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_173910-f21gmjzy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go  # 인터랙티브 차트\n",
    "\n",
    "# 출력 옵션\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Dataset\n",
    "# ---------------------------\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X): self.X = torch.FloatTensor(X)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx]}\n",
    "\n",
    "# ---------------------------\n",
    "# 2) 전처리\n",
    "# ---------------------------\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    all_df = all_df.drop(columns=[\"Fare_mean\"])\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"title\", \"name\"]\n",
    "    name_df = name_df.apply(lambda col: col.str.strip())\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    title_age_mean = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index()\n",
    "    title_age_mean.columns = [\"title\", \"title_age_mean\"]\n",
    "    all_df = pd.merge(all_df, title_age_mean, on=\"title\", how=\"left\")\n",
    "    all_df[\"title_age_mean\"] = all_df[\"title_age_mean\"].fillna(0)\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"title_age_mean\"]\n",
    "    all_df = all_df.drop([\"title_age_mean\"], axis=1)\n",
    "    all_df[\"Age\"] = all_df[\"Age\"].fillna(all_df[\"Age\"].mean())\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"] = all_df[\"alone\"].fillna(0)\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    main_titles = [\"Mr\", \"Miss\", \"Mrs\", \"Master\"]\n",
    "    all_df.loc[~all_df[\"title\"].isin(main_titles), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    for feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        all_df[feature] = le.fit_transform(all_df[feature])\n",
    "    return all_df\n",
    "\n",
    "# ---------------------------\n",
    "# 3) DataLoader (wandb.config.batch_size 사용)\n",
    "# ---------------------------\n",
    "def get_data():\n",
    "    train_data_path = os.path.join(os.getcwd(), \"train.csv\")\n",
    "    test_data_path = os.path.join(os.getcwd(), \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ---------------------------\n",
    "# 4) 모델/옵티마이저 (ELU만 사용)\n",
    "# ---------------------------\n",
    "class MyModel(nn.Module):\n",
    "    N_INPUT = 10\n",
    "    N_OUTPUT = 1\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.N_INPUT, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], self.N_OUTPUT),\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "    model = MyModel()  # ELU 고정\n",
    "    optimizer = optim.SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "# ---------------------------\n",
    "# 5) 학습 루프 (히스토리 수집)\n",
    "# ---------------------------\n",
    "def training_loop(model, optimizer, train_loader, val_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    train_hist, val_hist = [], []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        loss_train, correct_train, total_train = 0.0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x = batch['input']\n",
    "            y = batch['target'].float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            pred = (torch.sigmoid(out) > 0.5).long()\n",
    "            correct_train += (pred == y.long()).sum().item()\n",
    "            total_train += y.size(0)\n",
    "        train_loss_avg = loss_train / len(train_loader)\n",
    "        train_hist.append(train_loss_avg)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        loss_val, correct_val, total_val = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch['input']\n",
    "                y = batch['target'].float().unsqueeze(1)\n",
    "                out = model(x)\n",
    "                loss = loss_fn(out, y)\n",
    "                loss_val += loss.item()\n",
    "                pred = (torch.sigmoid(out) > 0.5).long()\n",
    "                correct_val += (pred == y.long()).sum().item()\n",
    "                total_val += y.size(0)\n",
    "        val_loss_avg = loss_val / len(val_loader)\n",
    "        val_hist.append(val_loss_avg)\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training loss\": train_loss_avg,\n",
    "            \"Validation loss\": val_loss_avg,\n",
    "            \"Training accuracy\": correct_train / total_train,\n",
    "            \"Validation accuracy\": correct_val / total_val,\n",
    "        })\n",
    "\n",
    "    return {\"train_loss_history\": train_hist, \"val_loss_history\": val_hist}\n",
    "\n",
    "# ---------------------------\n",
    "# 6) 메인: ELU 고정 + 배치사이즈 비교 (16/32/64/128)\n",
    "# ---------------------------\n",
    "def main(args):\n",
    "    current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,   # 초기값(루프에서 덮어씀)\n",
    "        'learning_rate': 1e-3,\n",
    "        'n_hidden_unit_list': [32, 16],\n",
    "        'compare_mode': 'batch_size',\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"titanic_survival_prediction\",\n",
    "        notes=\"ELU-only | Compare batch sizes (16/32/64/128) | Plotly + backup\",\n",
    "        tags=[\"titanic\", \"elu\", \"batchsize-compare\", \"plotly\"],\n",
    "        name=current_time_str,\n",
    "        config=config,\n",
    "        settings=wandb.Settings(_disable_stats=True),  # 시스템 성능 로깅 비활성화\n",
    "    )\n",
    "\n",
    "    batch_sizes = [16, 32, 64, 128]\n",
    "    train_hist_by_bs, val_hist_by_bs = {}, {}\n",
    "\n",
    "    for bs in batch_sizes:\n",
    "        # 배치 사이즈 갱신 → DataLoader 재생성\n",
    "        wandb.config.update({\"batch_size\": bs}, allow_val_change=True)\n",
    "        train_loader, val_loader = get_data()\n",
    "\n",
    "        # 모델/옵티마이저 생성 (ELU 고정)\n",
    "        model, opt = get_model_and_optimizer()\n",
    "\n",
    "        print(f\"\\n[Batch Size: {bs} | Activation: ELU]\")\n",
    "        metrics = training_loop(model, opt, train_loader, val_loader)\n",
    "        train_hist_by_bs[bs] = metrics[\"train_loss_history\"]\n",
    "        val_hist_by_bs[bs]   = metrics[\"val_loss_history\"]\n",
    "\n",
    "    # --- Plotly 그래프 (Media 탭)\n",
    "    epochs = list(range(1, wandb.config.epochs + 1))\n",
    "    color_map = {16: \"#1f77b4\", 32: \"#ff7f0e\", 64: \"#2ca02c\", 128: \"#d62728\"}\n",
    "\n",
    "    # Validation Loss\n",
    "    fig_val = go.Figure()\n",
    "    for bs in batch_sizes:\n",
    "        fig_val.add_trace(go.Scatter(\n",
    "            x=epochs, y=val_hist_by_bs[bs], mode=\"lines\",\n",
    "            name=f\"BS={bs}\", line=dict(color=color_map[bs], width=2)\n",
    "        ))\n",
    "    fig_val.update_layout(\n",
    "        title=\"Validation Loss by Batch Size (Activation = ELU)\",\n",
    "        xaxis_title=\"Epoch\", yaxis_title=\"Loss\",\n",
    "        legend_title=\"Batch Size\", template=\"plotly_white\"\n",
    "    )\n",
    "    wandb.log({\"val_loss_by_batchsize_plotly\": wandb.Plotly(fig_val)}, commit=True)\n",
    "\n",
    "    # Training Loss\n",
    "    fig_tr = go.Figure()\n",
    "    for bs in batch_sizes:\n",
    "        fig_tr.add_trace(go.Scatter(\n",
    "            x=epochs, y=train_hist_by_bs[bs], mode=\"lines\",\n",
    "            name=f\"BS={bs}\", line=dict(color=color_map[bs], width=2)\n",
    "        ))\n",
    "    fig_tr.update_layout(\n",
    "        title=\"Training Loss by Batch Size (Activation = ELU)\",\n",
    "        xaxis_title=\"Epoch\", yaxis_title=\"Loss\",\n",
    "        legend_title=\"Batch Size\", template=\"plotly_white\"\n",
    "    )\n",
    "    wandb.log({\"train_loss_by_batchsize_plotly\": wandb.Plotly(fig_tr)})\n",
    "\n",
    "    # --- 백업: Charts 탭 라인차트\n",
    "    val_plot_backup = wandb.plot.line_series(\n",
    "        xs=epochs,\n",
    "        ys=[val_hist_by_bs[b] for b in batch_sizes],\n",
    "        keys=[f\"BS={b}\" for b in batch_sizes],\n",
    "        title=\"Validation Loss by Batch Size (backup)\",\n",
    "        xname=\"Epoch\"\n",
    "    )\n",
    "    train_plot_backup = wandb.plot.line_series(\n",
    "        xs=epochs,\n",
    "        ys=[train_hist_by_bs[b] for b in batch_sizes],\n",
    "        keys=[f\"BS={b}\" for b in batch_sizes],\n",
    "        title=\"Training Loss by Batch Size (backup)\",\n",
    "        xname=\"Epoch\"\n",
    "    )\n",
    "    wandb.log({\n",
    "        \"val_loss_by_batchsize_backup\": val_plot_backup,\n",
    "        \"train_loss_by_batchsize_backup\": train_plot_backup,\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# ---------------------------\n",
    "# 7) 실행\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", action=argparse.BooleanOptionalAction, default=False)\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=500)\n",
    "    args = parser.parse_args([])\n",
    "    args.wandb = True  # Jupyter에서 강제 on\n",
    "    print(f\"[NOTE] WandB manually set to: {args.wandb}\")\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5641f-7256-4727-9931-04fa2c496564",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- for bs in batch_sizes : train_loader, val_loader = get_data()\n",
    "  : 배치사이즈를 바꿀 때마다 get_data() 다시 호출 => train_loader와 val_loader가 새 배치 크기로 형성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a82e3-0160-4d5f-be16-113de252e89f",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- batch size가 64, 128인 경우 수렴 속도는 느리고, Validation Loss도 높게 유지되므로 16, 32인 경우보다 성능이 낮다고 평가할 수 있음\n",
    "- batch size가 32인 경우 validation loss가 16인 경우보다 더 낮게 내려가므로 가장 좋은 성능의 batch size로 32 채택\n",
    "- 직접 보기에 batch size가 32인 경우가 불안정해보여 분석한 결과 전반적으로 batch size가 16인 경우의 validation loss보다 batch size가 32인인 validation loss가 더 낮은 평균을 보여주며, 모델을 더 오래 학습시킨다면 불안정성을 회복시킬 수 있을 거라는 예상이 나와 이를 채택했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7effed-427b-4ea7-9b97-de18e93030c1",
   "metadata": {},
   "source": [
    "## [요구사항 3]\t테스트\t및\tsubmission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62f6a13-2e0d-4f9e-b5bf-bade535fc76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTE] WandB manually set to: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-10-17_19-47-19</strong> at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/vqmfu6n6' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/vqmfu6n6</a><br> View project at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_194719-vqmfu6n6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\le993\\git\\link_dl\\_04_your_code\\wandb\\run-20251017_194857-zowqpxwi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/zowqpxwi' target=\"_blank\">2025-10-17_19-48-56</a></strong> to <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/zowqpxwi' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/zowqpxwi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved: submission_20251017_194959.csv (rows=418)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Best epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>Best val loss so far</td><td>███████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>LR</td><td>███████████████████████████████████████▁</td></tr><tr><td>Training accuracy</td><td>▁▁▁▄▄▄▅▄▄▄▅▅▄▅▅▄▅▅▆▅▅▅▆▅▆▆▅▆▆▇▆▇▇▅▆▇▇█▇▇</td></tr><tr><td>Training loss</td><td>████▇▇▇▇█▆▆▇▆▆▆▅▆▆▅▅▆▄▄▄▄▄▃▃▄▃▃▃▂▃▄▃▂▂▂▁</td></tr><tr><td>Validation accuracy</td><td>▂▃▂▂▂▅▅▄▄▁▄▄▄▄▅▄▄▅▆▅▅▄▅▅▅▃▄▅▄▅▆▅▆▅▆▆█▅▆▇</td></tr><tr><td>Validation loss</td><td>███████▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▅▄▄▄▃▄▃▄▃▃▃▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best epoch</td><td>500</td></tr><tr><td>Best val loss so far</td><td>0.5769</td></tr><tr><td>Epoch</td><td>500</td></tr><tr><td>LR</td><td>0.0005</td></tr><tr><td>Training accuracy</td><td>0.7486</td></tr><tr><td>Training loss</td><td>0.5148</td></tr><tr><td>Validation accuracy</td><td>0.67039</td></tr><tr><td>Validation loss</td><td>0.5769</td></tr><tr><td>best_epoch</td><td>500</td></tr><tr><td>best_val_loss</td><td>0.5769</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-10-17_19-48-56</strong> at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/zowqpxwi' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction/runs/zowqpxwi</a><br> View project at: <a href='https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction' target=\"_blank\">https://wandb.ai/le993939-korea-university-of-technology-and-education/titanic_survival_prediction</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_194857-zowqpxwi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset\n",
    "# ---------------------------\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx], 'target': self.y[idx]}\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "    \"\"\"test.csv처럼 라벨 없는 데이터용\"\"\"\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return {'input': self.X[idx]}\n",
    "\n",
    "# ---------------------------\n",
    "# 전처리 모듈\n",
    "# ---------------------------\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[all_df[\"Fare\"].isnull(), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    all_df = all_df.drop(columns=[\"Fare_mean\"])\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"title\", \"name\"]\n",
    "    name_df = name_df.apply(lambda col: col.str.strip())\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    title_age_mean = all_df[[\"title\", \"Age\"]].groupby(\"title\").median().round().reset_index()\n",
    "    title_age_mean.columns = [\"title\", \"title_age_mean\"]\n",
    "    all_df = pd.merge(all_df, title_age_mean, on=\"title\", how=\"left\")\n",
    "    all_df[\"title_age_mean\"] = all_df[\"title_age_mean\"].fillna(0)\n",
    "    all_df.loc[all_df[\"Age\"].isnull(), \"Age\"] = all_df[\"title_age_mean\"]\n",
    "    all_df = all_df.drop([\"title_age_mean\"], axis=1)\n",
    "    all_df[\"Age\"] = all_df[\"Age\"].fillna(all_df[\"Age\"].mean())\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"] = all_df[\"alone\"].fillna(0)\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    main_titles = [\"Mr\", \"Miss\", \"Mrs\", \"Master\"]\n",
    "    all_df.loc[~all_df[\"title\"].isin(main_titles), \"title\"] = \"other\"\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    for feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        all_df[feature] = le.fit_transform(all_df[feature])\n",
    "    return all_df\n",
    "\n",
    "def preprocess_all(train_df, test_df):\n",
    "    \"\"\"train/test를 합쳐 동일 전처리 적용 후, 다시 분리\"\"\"\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    for f in [get_preprocessed_dataset_1, get_preprocessed_dataset_2, get_preprocessed_dataset_3,\n",
    "              get_preprocessed_dataset_4, get_preprocessed_dataset_5, get_preprocessed_dataset_6]:\n",
    "        all_df = f(all_df)\n",
    "    return all_df\n",
    "\n",
    "# ---------------------------\n",
    "# DataLoaders\n",
    "# ---------------------------\n",
    "def get_train_val_loaders():\n",
    "    train_data_path = os.path.join(os.getcwd(), \"train.csv\")\n",
    "    test_data_path = os.path.join(os.getcwd(), \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df  = pd.read_csv(test_data_path)\n",
    "\n",
    "    all_df = preprocess_all(train_df, test_df)\n",
    "\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    val_size   = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=val_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def get_test_loader_and_ids():\n",
    "    \"\"\"test.csv 파트만 분리해 DataLoader와 PassengerId 반환\"\"\"\n",
    "    train_data_path = os.path.join(os.getcwd(), \"train.csv\")\n",
    "    test_data_path  = os.path.join(os.getcwd(), \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df  = pd.read_csv(test_data_path)\n",
    "    test_ids = test_df[\"PassengerId\"].values  # 전처리 전 보관\n",
    "\n",
    "    all_df = preprocess_all(train_df, test_df)\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True).values\n",
    "    test_dataset = TitanicTestDataset(test_X)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=wandb.config.batch_size)\n",
    "    return test_loader, test_ids\n",
    "\n",
    "# ---------------------------\n",
    "# Model / Optim\n",
    "# ---------------------------\n",
    "class MyModel(nn.Module):\n",
    "    N_INPUT = 10\n",
    "    N_OUTPUT = 1\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.N_INPUT, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], self.N_OUTPUT),\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "    model = MyModel()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "# ---------------------------\n",
    "# Train loop with best-checkpoint\n",
    "# ---------------------------\n",
    "def training_loop(model, optimizer, train_loader, val_loader, ckpt_path=\"best_model.pt\"):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn  = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch    = -1\n",
    "    train_hist, val_hist = [], []\n",
    "\n",
    "    # 스케줄러\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=20)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        loss_train, correct_train, total_train = 0.0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x = batch['input']\n",
    "            y = batch['target'].float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            pred = (torch.sigmoid(out) > 0.5).long()\n",
    "            correct_train += (pred == y.long()).sum().item()\n",
    "            total_train += y.size(0)\n",
    "        train_loss_avg = loss_train / len(train_loader)\n",
    "        train_hist.append(train_loss_avg)\n",
    "\n",
    "        # Val\n",
    "        model.eval()\n",
    "        loss_val, correct_val, total_val = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch['input']\n",
    "                y = batch['target'].float().unsqueeze(1)\n",
    "                out = model(x)\n",
    "                loss = loss_fn(out, y)\n",
    "                loss_val += loss.item()\n",
    "                pred = (torch.sigmoid(out) > 0.5).long()\n",
    "                correct_val += (pred == y.long()).sum().item()\n",
    "                total_val += y.size(0)\n",
    "        val_loss_avg = loss_val / len(val_loader)\n",
    "        val_hist.append(val_loss_avg)\n",
    "\n",
    "        train_acc = correct_train / total_train\n",
    "        val_acc   = correct_val   / total_val\n",
    "\n",
    "        # BEST 체크포인트 저장 (검증 손실 최저 시)\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "        # 스케줄러 업데이트\n",
    "        scheduler.step(val_loss_avg)\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training loss\": train_loss_avg,\n",
    "            \"Validation loss\": val_loss_avg,\n",
    "            \"Training accuracy\": train_acc,\n",
    "            \"Validation accuracy\": val_acc,\n",
    "            \"Best val loss so far\": best_val_loss,\n",
    "            \"Best epoch\": best_epoch,\n",
    "            \"LR\": optimizer.param_groups[0][\"lr\"],\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"train_loss_history\": train_hist,\n",
    "        \"val_loss_history\": val_hist,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"ckpt_path\": ckpt_path,\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Test inference → submission.csv\n",
    "# ---------------------------\n",
    "def predict_test_and_save_csv(model, ckpt_path, out_csv=\"submission.csv\"):\n",
    "    # best checkpoint 로드\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "\n",
    "    test_loader, test_ids = get_test_loader_and_ids()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch['input']\n",
    "            logits = model(x)\n",
    "            probs = torch.sigmoid(logits).squeeze(1)   # [N]\n",
    "            p = (probs > 0.5).long().cpu().numpy()\n",
    "            preds.extend(p.tolist())\n",
    "\n",
    "    submission = pd.DataFrame({\"PassengerId\": test_ids, \"Survived\": preds})\n",
    "    submission.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] Saved: {out_csv} (rows={len(submission)})\")\n",
    "    return out_csv\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "def main(args):\n",
    "    current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': 32,                 # BS=32 고정\n",
    "        'learning_rate': 1e-3,\n",
    "        'n_hidden_unit_list': [32, 16],\n",
    "        'setup': 'ELU + BS=32',\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"titanic_survival_prediction\",\n",
    "        notes=\"ELU + BS=32 | best val-loss checkpoint → test inference → submission\",\n",
    "        tags=[\"titanic\", \"elu\", \"bs32\", \"best-checkpoint\", \"submission\"],\n",
    "        name=current_time_str,\n",
    "        config=config,\n",
    "        settings=wandb.Settings(_disable_stats=True),\n",
    "    )\n",
    "\n",
    "    train_loader, val_loader = get_train_val_loaders()\n",
    "    model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "    ckpt_path = \"best_model.pt\"\n",
    "    metrics = training_loop(model, optimizer, train_loader, val_loader, ckpt_path=ckpt_path)\n",
    "\n",
    "    #  훈련 종료 후: best checkpoint로 test 예측 → submission.csv\n",
    "    out_csv = f\"submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    predict_test_and_save_csv(model, metrics[\"ckpt_path\"], out_csv=out_csv)\n",
    "\n",
    "    wandb.summary[\"best_epoch\"] = metrics[\"best_epoch\"]\n",
    "    wandb.summary[\"best_val_loss\"] = metrics[\"best_val_loss\"]\n",
    "    wandb.summary[\"submission_csv\"] = out_csv\n",
    "\n",
    "    # Charts 탭에 손실 곡선 로그\n",
    "    epochs = list(range(1, wandb.config.epochs + 1))\n",
    "    wandb.log({\n",
    "        \"val_loss_curve\": wandb.plot.line_series(xs=epochs, ys=[metrics[\"val_loss_history\"]],\n",
    "                                                 keys=[\"Validation\"], title=\"Validation Loss\", xname=\"Epoch\"),\n",
    "        \"train_loss_curve\": wandb.plot.line_series(xs=epochs, ys=[metrics[\"train_loss_history\"]],\n",
    "                                                   keys=[\"Training\"], title=\"Training Loss\", xname=\"Epoch\"),\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", action=argparse.BooleanOptionalAction, default=False)\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=500)\n",
    "    args = parser.parse_args([])  # 노트북에서 인자 없이 실행\n",
    "    args.wandb = True\n",
    "    print(f\"[NOTE] WandB manually set to: {args.wandb}\")\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54047051-0d1b-4647-9654-0d65a454382f",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- ReduceLROnPlateau : LR 스케줄러로 검증 손실 정체 시 학습률 감소\n",
    "- if val_loss_avg < best_val_loss : 검증 손실이 사상 최저일 때 베스트 체크포인트 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5b5d0-fd42-4aa9-b222-8feef5f6a1b9",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- 검증 손실 최저가 마지막 에포크에 발생했으므로 epoch 500 시점에서 submission.csv 형성\n",
    "- batch size에서 16과 32의 사이, 32와 64의 사이를 측정하면 더 나은 결과를 얻을 수 있는 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7e265-7e4c-4b76-ac4b-0fc51732c708",
   "metadata": {},
   "source": [
    "## [요구사항 4]\tsubmission.csv제출 및 등수확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ca40c-3827-4c2d-bf42-f18ceda3eecb",
   "metadata": {},
   "source": [
    "![요구사항 4 이미지](https://github.com/tongwu2021/deep-learning-homework/blob/main/hw_2_%EC%9A%94%EA%B5%AC%EC%82%AC%ED%95%AD%204.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1062b2c-3955-4c59-8bdd-6413f03564ad",
   "metadata": {},
   "source": [
    "## 숙제 후기\n",
    "wandb 그래프 비교분석을 한 차트 안에서 하는 걸 추천한다고 했기에 이를 처음 시도했을 때, 한 차트 안에 있는 여러 그래프가 서로 구별하기 힘들게 나와 놀랐었다.\n",
    "코드를 기록하고 실행하면서 많은 오류가 발생했고 이를 고치는데 꽤 많은 시간을 사용한 것 같다. 음, 요약하자면 중간고사 공부보다 이 과제를 더 열심히 한 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
