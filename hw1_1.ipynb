{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38f839f-407b-4769-87a2-7b5dff4e613d",
   "metadata": {},
   "source": [
    "## a_tersor_initialization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:32:52.637187Z",
     "start_time": "2025-09-09T07:32:51.226447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc2e2f-cb44-49b9-8cd0-ad793d4c5153",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.Tensor에서 대문자를 사용한 Tensor를 사용 => float32 타입의 텐서를 생성\n",
    "- device='cpu'를 통해 텐서가 cpu에 저장\n",
    "- t1.requires_grad의 결과는 PyTorch에서 기본적으로 텐서를 만들 때 자동 미분 기능이 꺼져 있기 때문\n",
    "- 3개의 요소가 있는 1차원 텐서 => torch.Size([3])\n",
    "- size()와 shape는 같은 역할을 하므로 결과가 같음\n",
    "- .cpu()는 텐서의 위치를 cpu로 이동시키는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5490cd8-f705-4bfd-9de0-fe81ccc2c8b2",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- PyTorch에서 기본적으로 텐서를 만들 때 자동 미분 기능이 같이 존재한다는 사실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b12efd-d39c-4a6e-8aeb-bd70af1b295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f0538-8af8-4af6-be7d-d0bcb81c9797",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.tensor에서 소문자를 사용한 tensor를 사용 => 데이터 타입을 자동으로 추론\n",
    "  => [1, 2, 3]은 정수 리스트이므로 int64 형태로 만들어짐\n",
    "- 나머지는 위의 해석과 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4274e55-8d26-4838-8e6f-a2cf843b8eee",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- torch.tensor과 torch.Tensor의 t의 대문자 여부에 따라 데이터 타입이 달라진다는 사실과 이를 주의깊게 봐야된다는 사실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da69bb0-c0cc-4121-8a05-96d6d8007c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = torch.Size([]) 0\n",
      "a2 = torch.Size([1]) 1\n",
      "a3 = torch.Size([5]) 1\n",
      "a4 = torch.Size([5, 1]) 2\n",
      "a5 = torch.Size([3, 2]) 2\n",
      "a6 = torch.Size([3, 2, 1]) 3\n",
      "a7 = torch.Size([3, 1, 2, 1]) 4\n",
      "a8 = torch.Size([3, 1, 2, 3]) 4\n",
      "a9 = torch.Size([3, 1, 2, 3, 1]) 5\n",
      "a10 = torch.Size([4, 5]) 2\n",
      "a10 = torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 65\u001b[0m\n\u001b[0;32m     57\u001b[0m a10 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([                 \u001b[38;5;66;03m# shape: torch.Size([4, 1, 5]), ndims(=rank): 3\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     59\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     60\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     61\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     62\u001b[0m ])\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma10 =\u001b[39m\u001b[38;5;124m\"\u001b[39m,a10\u001b[38;5;241m.\u001b[39mshape, a10\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m---> 65\u001b[0m a11 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001b[39;49;00m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(\"a1 =\",a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(\"a2 =\",a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(\"a3 =\",a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(\"a4 =\",a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(\"a5 =\",a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(\"a6 =\",a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(\"a7 =\",a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(\"a8 =\",a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(\"a9 =\",a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(\"a10 =\",a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(\"a10 =\",a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2eb26e-08b8-48bb-a3c5-0f60b5b4d677",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- .shape는 각 차원의 크기를 알려주며, .ndim은 차원의 개수를 알려준다.\n",
    "- tensor()안에 []이 들어가지 않은 건 차원이 없다는 것 => 이는 a1의 shape 결과 torch.Size([])로 이어지며 이는 차원이 없다는 것을 의미한다\n",
    "- 1차원은 dim0, 2차원은 dim1 순으로 숫자가 늘어난다.\n",
    "- tensor()를 만들 때, 기본적으로 같은 차원에 있는 데이터의 길이가 같아야 한다는 가정으로 생성된다. 그러나 a11은 4번째 차원에서 데이터의 길이가 3, 2로 각각 달라 오류가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68543243-1db7-4bb7-a52c-2720e0124f08",
   "metadata": {},
   "source": [
    "### 코드 추가\n",
    "- 결과의 명확성을 추가하기 위해 print() 안에 코드 일부를 추가했다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581211c9-771e-4be5-9482-bfb3c9f97653",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- tensor()를 만들 때, 기본적으로 같은 차원에 있는 데이터의 길이가 같아야 한다는 가정으로 생성된다는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae2db6-e19e-4c62-80a6-4b157465067a",
   "metadata": {},
   "source": [
    "## b_tensor_initialization_copy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4c9f13d-de6b-4f09-81db-e0319bc4cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 : tensor([1., 2., 3.])\n",
      "t2 : tensor([1, 2, 3])\n",
      "t3 : tensor([1, 2, 3])\n",
      "t4 : tensor([1., 2., 3.])\n",
      "t5 : tensor([1, 2, 3])\n",
      "t6 : tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(\"t1 :\",t1)\n",
    "print(\"t2 :\",t2)\n",
    "print(\"t3 :\",t3)\n",
    "\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(\"t4 :\",t4)\n",
    "print(\"t5 :\",t5)\n",
    "print(\"t6 :\",t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb36bf-f0cf-49c6-ab37-9afa2a008460",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.Tensor()과 torch.tensor() 모두 복사본을 형성하여 텐서를 생성한다. 이는 원본 list 또는 NumPy 배열의 내부 데이터가 바뀌어도 텐서에 영향을 끼치지 않는다는 걸 알려준다.\n",
    "- as_tensor()은 PyTorch가 NumPy 배열을 감지할 경우 데이터를 공유하지만, 리스트는 공유하지 않고 복사본을 형성한다. t3와 t6의 결과를 보면 이를 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353ebd8-8312-44b5-9b08-a06bd5fbc015",
   "metadata": {},
   "source": [
    "### 코드 추가\n",
    "- 결과의 명확성을 추가하기 위해 print() 안에 코드 일부를 추가했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162df3ee-4a20-4488-b995-9f39f066541f",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- NumPy 배열을 사용한 텐서가 as_tensor() 함수를 사용했을 때만 데이터를 공유하고, 나머지 경우의 수는 다 복제본을 생성한다는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41794aff-24be-47c5-ae97-e06b4d1cb961",
   "metadata": {},
   "source": [
    "## c_tensor_initialization_constant_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b030516-4415-4cce-9180-3143ee0de50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 : tensor([1., 1., 1., 1., 1.])\n",
      "t1_like : tensor([1., 1., 1., 1., 1.])\n",
      "t2 : tensor([0., 0., 0., 0., 0., 0.])\n",
      "t2_like : tensor([0., 0., 0., 0., 0., 0.])\n",
      "t3 : tensor([4.3024e-22, 1.1603e-42, 0.0000e+00, 0.0000e+00])\n",
      "t3_like : tensor([0., 0., 0., 0.])\n",
      "t4 : tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "t5 : tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n",
      "t6 : tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(\"t1 :\",t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(\"t1_like :\",t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(\"t2 :\",t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(\"t2_like :\",t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(\"t3 :\",t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(\"t3_like :\",t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "t4 = torch.eye(n=3)\n",
    "print(\"t4 :\",t4)\n",
    "\n",
    "t5 = torch.eye(3,5)\n",
    "print(\"t5 :\",t5)\n",
    "\n",
    "t6 = torch.eye(5,3)\n",
    "print(\"t6 :\",t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d48668-e38e-43fe-84bf-ebedc971122e",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.ones()는 텐서의 값을 모두 1.0으로 설정한다.\n",
    "- torch.zeros()는 텐서의 값을 모두 0.0으로 설정한다.\n",
    "- torch.empty()는 초기화를 하지 않은 메모리 공간을 그대로 사용하기 때문에 모든 값이 랜덤이다.\n",
    "- torch.eye(n)는 n*n 단위 행렬을 생성한다. dtype는 기본적으로 float32 형태이다. 대각선에는 1.0, 그 외의 값은 0.0의 값을 가진다.\n",
    "- torch.eye(n,m)를 통해 n*m 단위 행렬을 생성할 수 있다.\n",
    "  대각선에는 1.0, 그 외의 값은 0.0의 값을 가지며, 1.0을 가진 값의 개수를 알고 싶다면, n과 m 중 낮은 값을 제시하면 된다.\n",
    "- size=(n,)는 size=n과 같은 의미를 가진다.\n",
    "- ones_like(input=x)는 x 텐서와 같은 size와 dtype을 가지면서, 모든 값이 텐서인 1을 생성한다. 다만 새롭게 생성되는 텐서이기 때문에 x와는 다른 독립적인 텐서이다.\n",
    "- zeros_like(input=x)도 ones_like(input=x)와 역할이 같으며, 모든 값이 텐서인 0을 생성한다는 점만 다르다.\n",
    "- empty_like(input=x)도 ones_like(input=x)와 역할이 같으며, 모든 값이 랜덤이라는 점만 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce30f05-b1b7-4b90-a070-3125aec71e48",
   "metadata": {},
   "source": [
    "### 코드 추가\n",
    "- torch.eye()에서 길이가 다른 단위 행렬의 텐서에서 결과값이 어떻게 출력되는지를 이해하기 위해 t5, t6에 관한 코드를 추가했다.\n",
    "- 결과의 명확성을 추가하기 위해 print() 안에 코드 일부를 추가했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eeec17-ef1c-4cb3-87fc-469f33e7e383",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- torch.eye() 함수를 사용할 때 텐서의 각 차원 크기가 다를 때 1.0의 값의 위치와 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98190785-368c-4c6b-84fd-eb5810b45f0b",
   "metadata": {},
   "source": [
    "## d_tensor_initialization_random_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5af79b1-8bd3-4471-b39b-c96fd8d2bdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 : tensor([[17, 12]])\n",
      "t2 : tensor([[0.0453, 0.5035, 0.9978]])\n",
      "------------------------------\n",
      "t2_plus1 tensor([[1.1652, 2.0787, 0.5110]])\n",
      "t2_plus2 tensor([[2.1384, 2.4759, 2.7481]])\n",
      "t2_plus3 tensor([[2.1084, 3.5187, 4.5408]])\n",
      "------------------------------\n",
      "t3 : tensor([[ 0.7500,  0.3572, -1.1860]])\n",
      "------------------------------\n",
      "t3_plus1 : tensor([[-1.9487, -7.7143, -4.4621]])\n",
      "t3_plus2 : tensor([[4.2702, 2.1513, 1.3428]])\n",
      "t3_plus3 : tensor([[ 2.1347,  4.0381, -3.1737]])\n",
      "------------------------------\n",
      "t4 : tensor([[10.0326,  8.0829],\n",
      "        [11.1867,  8.9653],\n",
      "        [11.6391,  9.4500]])\n",
      "t5 : tensor([0.0000, 2.5000, 5.0000])\n",
      "------------------------------\n",
      "t5_plus1 : tensor([5.0000, 2.5000, 0.0000])\n",
      "t5_plus2 : tensor([5., 5., 5.])\n",
      "------------------------------\n",
      "t6 : tensor([0, 1, 2, 3, 4])\n",
      "------------------------------\n",
      "t6_plus1 : tensor([1, 2, 3, 4])\n",
      "t6_plus2 : tensor([5, 4, 3, 2])\n",
      "------------------------------\n",
      "##############################\n",
      "random1 : tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "random2 : tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "random3 : tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "random4 : tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(\"t1 :\",t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(\"t2 :\",t2)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t2_plus1 = torch.rand(size=(1, 3))*3\n",
    "print(\"t2_plus1\",t2_plus1)\n",
    "\n",
    "t2_plus2 = torch.rand(size=(1, 3)) + 2\n",
    "print(\"t2_plus2\",t2_plus2)\n",
    "\n",
    "t2_plus3 = torch.rand(size=(1, 3))*3 + 2\n",
    "print(\"t2_plus3\",t2_plus3)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(\"t3 :\",t3)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t3_plus1 = torch.randn(size=(1, 3)) * 4\n",
    "print(\"t3_plus1 :\",t3_plus1)\n",
    "\n",
    "t3_plus2 = torch.randn(size=(1, 3)) + 2\n",
    "print(\"t3_plus2 :\",t3_plus2)\n",
    "\n",
    "t3_plus3 = torch.randn(size=(1, 3))*4 + 2\n",
    "print(\"t3_plus3 :\",t3_plus3)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(\"t4 :\",t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(\"t5 :\",t5)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t5_plus1 = torch.linspace(start=5.0, end=0.0, steps=3)\n",
    "print(\"t5_plus1 :\",t5_plus1)\n",
    "\n",
    "t5_plus2 = torch.linspace(start=5.0, end=5.0, steps=3)\n",
    "print(\"t5_plus2 :\",t5_plus2)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "print(\"t6 :\",t6)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t6_plus1 = torch.arange(1,5)\n",
    "print(\"t6_plus1 :\",t6_plus1)\n",
    "\n",
    "t6_plus2 = torch.arange(5,1,-1)\n",
    "print(\"t6_plus2 :\",t6_plus2)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(\"random1 :\",random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(\"random2 :\",random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(\"random3 :\",random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(\"random4 :\",random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85d3fd-de38-4b22-a867-f8ecbcb016c4",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.randint() : 지정된 범위 내에서 무작위 정수를 생성하는 함수\n",
    "- t1의 low=10, high=20 : 10 이상 20 미만\n",
    "- torch.rand() : 기본적으로 0.0 이상 1.0 미만에서 균등 분포를 기준으로 난수를 생성\n",
    "- torch.rand() * n : 0.0 이상 1.0 * n 미만에서 균등 분포를 기준으로 난수를 생성\n",
    "- torch.rand() + m : 0.0 + m 이상 1.0 + m 미만에서 균등 분포를 기준으로 난수를 생성\n",
    "- torch.randn() : 정규 분포에 따라 난수를 생성하는 함수. 기본적으로 평균은 0, 표준편차가 1이며, 대부분 -3 ~ +3 사이의 값이 출력됨.\n",
    "- torch.randn() * std : 표준편차가 std인 정규 분포\n",
    "- torch.randn() + mean : 평균이 mean인 정규 분포\n",
    "- torch.normal() : 정규 분포에 따라 난수를 생성하는 함수. torch.randn()과 달리 직접 평균과 표준편차를 지정해줘야 함. t4에서 mean이 평균, std가 표준편차를 지정해주는 값임.\n",
    "- torch.linspace() : 시작값과 끝값을 균등하게 나누어 결과값을 출력함. t5에서 시작값이 0.0, 끝값이 5.0에서 3분할로 나누었으므로, [0.0,2.5,5.0]의 결과가 나온다.\n",
    "- torch.arange(n) : 기본적으로 0 이상 n 미만의 정수를 출력하는 함수.\n",
    "- torch.manual_seed() : 시드를 기반으로 난수를 만들며, 같은 시드를 설정하면 항상 같은 순서와 같은 난수를 생성한다. random1과 random3, random2와 random4의 출력 결과값이 같다는 것이 이를 알려준다. 머신러닝, 딥러닝에서 같은 모델을 여러 번 돌릴 때, 결과가 달라지면 문제가 생길 수 있다. 이에 실험 재현, 디버깅, 결과 비교에 시드 고정은 필수이므로 이 함수가 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b6752-5197-428f-ad07-f0da570214bd",
   "metadata": {},
   "source": [
    "### 코드 추가\n",
    "- 추가된 코드를 구분하기 위해 print(\"-\" * 30)을 추가했다.\n",
    "- torch.rand()의 범위를 바꾸는 방법과 결과 이해를 위해 t2_plus1~3에 관한 코드를 추가했다.\n",
    "- torch.randn()의 범위를 바꾸는 방법과 결과 이해를 위해 t3_plus1~3에 관한 코드를 추가했다.\n",
    "- torch.linspace()의 시작값과 끝값이 달라지는 방향에 따른 결과를 이해하기 위해 t5_plus1~2에 관한 코드를 추가했다.\n",
    "- torch.arrange()에서 시작값, 끝값, 간격 지정에 따라 달라지는 결과를 이해하기 위해 t6_plus1~2에 관한 코드를 추가했다.\n",
    "- 결과의 명확성을 추가하기 위해 print() 안에 코드 일부를 추가했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a5f99-23fe-4cbe-b301-9e830dd6248f",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- rand(), randn() 함수에 연산자가 추가되었을 때 나타나는 결과\n",
    "- 같은 시드를 설정하면 항상 같은 순서와 같은 난수를 생성하는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893c6de-9f95-45ab-93f0-5243e5025446",
   "metadata": {},
   "source": [
    "## e_tensor_type_conversion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a2ab34c-9568-419d-85b1-c7b924d09d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.dtype : torch.float32\n",
      "------------------------------\n",
      "a : tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "------------------------------\n",
      "b : tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "c : tensor([[15.8734,  4.0635,  5.6361],\n",
      "        [11.7664,  0.4109,  0.7274]], dtype=torch.float64)\n",
      "d : tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "------------------------------\n",
      "double_d.dtype : torch.float64\n",
      "short_e.dtype : torch.int16\n",
      "double_d.dtype : torch.float64\n",
      "short_e.dtype : torch.int16\n",
      "double_d.dtype : torch.float64\n",
      "short_e.dtype : torch.int16\n",
      "------------------------------\n",
      "double_d.dtype : torch.float64\n",
      "short_e.dtype : torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(\"a.dtype :\",a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(\"-\" * 30)\n",
    "print(\"a :\",a)\n",
    "print(\"-\" * 30)\n",
    "print(\"b :\",b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(\"c :\",c)\n",
    "\n",
    "d = b.to(torch.int32)\n",
    "print(\"d :\",d)\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"double_d.dtype :\",double_d.dtype)\n",
    "print(\"short_e.dtype :\",short_e.dtype)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "print(\"double_d.dtype :\",double_d.dtype)\n",
    "print(\"short_e.dtype :\",short_e.dtype)\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "print(\"double_d.dtype :\",double_d.dtype)\n",
    "print(\"short_e.dtype :\",short_e.dtype)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"double_d.dtype :\",double_d.dtype)\n",
    "print(\"short_e.dtype :\",short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbea09-3296-45d2-9679-d1cebf00a932",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.ones()는 텐서의 값을 모두 1.0으로 설정한다. 즉, 기본적으로 dtype은 float32이다. 함수 안에 dtype을 따로 설정할 수 있다. 이는 b에 관한 코드를 보면 알 수 있다.\n",
    "- torch.rand() * n : 0.0 이상 1.0 * n 미만에서 균등 분포를 기준으로 난수를 생성\n",
    "- torch.rand() 또한 dtype를 따로 설정할 수 있다.\n",
    "- .to() : 데이터 타입, 디바이스(CPU/GPU), layout 등을 변경할 수 있는 함수\n",
    "- double_d와 관련된 코드는 float64 형태를, short_e와 관련된 코드는 int16 형태로 텐서를 생성하는 걸 보여준다.\n",
    "- 서로 다른 dtype을 연산하면 더 정밀한 쪽으로 변환된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235e83e-a2c7-4b07-a565-7214c15b119e",
   "metadata": {},
   "source": [
    "### 코드 추가\n",
    "- 추가된 코드를 구분하기 위해 print(\"-\" * 30)을 추가했다.\n",
    "- 결과의 명확성을 추가하기 위해 print() 안에 코드 일부를 추가했다.\n",
    "- a와 b의 출력값을 비교하기 위해 print(a)를 추가했다.\n",
    "- double_d와 short_e의 타입이 정확히 나오는지 확인하기 위해 print()문을 추가했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdee64d-078a-4737-b250-415fd593c5b9",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- 텐서 자체의 결과는 tensor()로 결과가 출력되고, 텐서의 특징과 관련된 결과는 torch로 출력되는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e897d-dea0-448e-b01c-49ab64028ce6",
   "metadata": {},
   "source": [
    "## f_tensor_operations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6c1056-8689-4f98-99e8-15d1b8814661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425cb65-b4bb-4bf8-9c07-c86a35aa3661",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.add()는 element-wise 덧셈으로 t3[i][j] = t1[i][j] + t2[i][j] 방식으로 작동하며, 두 텐서의 덧셈 '+'도 같은 방식으로 작동한다.\n",
    "- torch.sub()는 element-wise 뺄셈으로 t5[i][j] = t1[i][j] - t2[i][j] 방식으로 작동하며, 두 텐서의 뺄셈 '-'도 같은 방식으로 작동한다.\n",
    "- torch.mul()는 element-wise 곱셈으로 t7[i][j] = t1[i][j] * t2[i][j] 방식으로 작동하며, 두 텐서의 곱셈 '*'도 같은 방식으로 작동한다.\n",
    "- torch.div()는 element-wise 나눗셈으로 t9[i][j] = t1[i][j] / t2[i][j] 방식으로 작동하며, 두 텐서의 나눗셈 '/'도 같은 방식으로 작동한다.\n",
    "- 위 네 개의 함수 모두 Element-wise operations과 Broadcasting을 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44434e2-5e4e-463c-8f8c-e1a70dbbf9e1",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- 함수를 간단한 연산자로 줄여쓰는 게 코드를 보기 편하다는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b611b9-615c-4fff-bdac-2bbab4610f52",
   "metadata": {},
   "source": [
    "## g_tensor_operations_mm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9f546c-14be-4c8d-b320-52063b2b838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[-1.9101,  0.6684],\n",
      "        [ 1.4764, -0.9888]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n",
      "------------------------------\n",
      "tensor([[-1.5126,  2.3840,  1.1192,  0.9393,  0.8394],\n",
      "        [-2.6540, -2.3992, -2.5322,  1.2110,  0.6262],\n",
      "        [-2.1623, -0.6121, -1.0223,  1.5126,  1.9039]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())\n",
    "print(\"-\" * 30)\n",
    "print(t7[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f17c78-5591-49d1-939f-86af16dd4a4a",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.dot()은 1차원 벡터 내적으로 x=(a,b), y=(c,d)일 때 result = a*c + b*d로 결과값은 스칼라, 즉 크기가 없는 0차원 텐서로 나타난다.\n",
    "- torch.mm()은 2차원 텐서의 행렬 곱셈으로 n×m 크기의 텐서와, m×p 크기의 텐서가 있으면 nxp 크기의 텐서 결과값이 나타난다.\n",
    "- 위 코드의 torch.mm()에서 각 원소의 곱셈 : t4[0][0] = t2[0] 벡터와 t3의 0번 열 벡터의 내적, t4[0][1] = t2[0] 벡터와 t3의 1번 열 벡터의 내적 등 방식으로 계산된다.\n",
    "- torch.bmm()은 3차원 텐서의 배치 행렬 곱셈으로 b×n×m 크기의 텐서와, b×m×p 크기의 텐서가 있으면 b×n×p 크기의 텐서 결과값이 나타난다.\n",
    "- 위 코드의 torch.bmm()에서 각 원소의 곱셈 : for i in range(10): t7[i] = torch.mm(t5[i], t6[i]) 방식으로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e66ff-c7b2-457b-8ee3-342d5699af92",
   "metadata": {},
   "source": [
    "### 코드 추가\n",
    "- torch.bmm()의 결과값 텐서의 원소를 확인해보기 위해 print()문을 추가했다.\n",
    "- 추가된 코드를 구분하기 위해 print(\"-\" * 30)을 추가했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e33791-e668-4263-b520-066f9b21a343",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- n×m 크기의 텐서와, m×p 크기의 텐서가 있으면 nxp 크기의 텐서 결과값\n",
    "- b×n×m 크기의 텐서와, b×m×p 크기의 텐서가 있으면 b×n×p 크기의 텐서 결과값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a5583-e09b-4358-91a7-0e8dfb9b7d5f",
   "metadata": {},
   "source": [
    "## h_tensor_operations_matmul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfe8864-31c0-4749-9416-d4483fd8cd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3280a49-3ffc-4808-9e30-97cf1c1fa8bd",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.matmul()은 다차원 텐서 행렬 곱을 유연하게 처리할 수 있는 함수로, dot product, matrix-vector product, matrix-matrix product(mm), batch-matrix-matrix product(bmm)를 자동으로 처리할 수 있다.\n",
    "- 배열 끝에 있는 크기 1 차원은 자동으로 생략해서 취급"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df63c6-0d42-4cdc-b185-c822e48efb1b",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- 배열 끝에 있는 크기 1 차원은 자동으로 생략해서 취급"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fadb94-1706-44ac-bcb2-aa0b8e37dc9b",
   "metadata": {},
   "source": [
    "## i_tensor_broadcasting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb8edb1-c5b5-48d6-880b-37d4a5b78f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) * 2.0 = \n",
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[ 0,  1],\n",
      "        [ 2,  4],\n",
      "        [10, 10]]) - tensor([4, 5]) =\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) + 2.0 =\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) - 2.0 =\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) * 2.0 =\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) / 2.0 =\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(f\"{t1} * {t2} = \")\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(f\"{t3} - {t4} =\")\n",
    "print(t3 - t4)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(f\"{t5} + 2.0 =\")  # t5.add(2.0)\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(f\"{t5} - 2.0 =\")\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(f\"{t5} * 2.0 =\")\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(f\"{t5} / 2.0 =\")\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3438f2d-5e4d-4858-b210-7e7d1c310499",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- Element-wise operations과 Broadcasting 예시를 보여주는 것으로 + - * / 연산자와 add/sub/mul/div 메서드는 동일 동작한다는 점도 보여준다.\n",
    "- normalize 함수는 텐서의 모든 원소를 255로 나누는 정규화 함수 역할을 한다.\n",
    "- 이미지 정규화(normalization)는 일반적인 RGB 이미지의 픽셀 값 범위를 0-255에서 0-1 사이로 스케일링하는 것이다.\n",
    "- Broadcasting에서 두 차원이 같거나, 한쪽 차원의 크기가 1이면 작동할 수 있다. 예를 들어 [2,3]+[4,2] 크기의 계산은 오류가 발생한다.\n",
    "- 누락된 앞쪽 차원이나 뒤쪽 차원의 크기를 1로 간주해 맞춘다.\n",
    "- torch.pow()는 원소별 거듭제곱의 함수이며, element-wise과 Broadcasting을 지원한다. ** 연산자가 이와 같은 표현이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ae741-b2cd-48c3-b1cd-4435acaa2076",
   "metadata": {},
   "source": [
    "### 코드 추가\n",
    "- 결과의 명확성을 추가하기 위해 print() 안에 코드 일부를 추가했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44759e57-e540-4c67-b525-e7caaf31a713",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- 이미지 정규화(normalization)는 일반적인 RGB 이미지의 픽셀 값 범위를 0-255에서 0-1 사이로 스케일링하는 것\n",
    "- Broadcasting에서 두 차원이 같거나, 한쪽 차원의 크기가 1이면 작동할 수 있는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6efb03-177f-484f-80b2-1b4ef11b824d",
   "metadata": {},
   "source": [
    "## j_tensor_indexing_slicing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be7031f-810b-4ee2-bf06-521290862965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2c172-a675-451a-b829-1a628939ecab",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- x[n]은 x 텐서의 (n+1) 행 전체를 의미한다.\n",
    "- x[:, 1]에서 :은 모든 행, 1은 두 번째 열을 의미하고, x[:, -1]은 모든 행에서 마지막 열을 의미한다.\n",
    "- x[n:]은 (n+1)번째 행부터 끝까지를 의미한다.\n",
    "- z[:m]은 m번째 행까지를 의미한다.\n",
    "- 즉 z[n:m]은 (n+1)번째 행부터 m번째 행까지를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea896e-aaee-4838-afc8-40af08d1a962",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- 이상 미만의 범위로 텐서의 인덱스를 가져오는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77865ad-92c1-430f-a6eb-887d5fc6b15f",
   "metadata": {},
   "source": [
    "## k_tensor_reshaping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3faefb7-dc6a-43ea-bbed-79d396eaeb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ddbe8c-b378-46b1-b1e5-775d9789075a",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- view()는 텐서의 데이터를 공유하면서 텐서의 크기를 재구성한다. 새로운 텐서의 총 요소 수는 원본과 같아야 하고, 메모리에서 연속적인(contiguous) 텐서에만 사용할 수 있다.\n",
    "- reshape()는 view()와 비슷하게 텐서의 크기를 변경하지만, 데이터가 메모리에서 연속적이지 않아도 작동한다. 필요한 경우 데이터를 복사하여 새로운 메모리 공간에 재배치하므로, view()보다 유연하게 사용할 수 있다.\n",
    "- squeeze()는 크기가 1인 차원을 없애는 함수로, 지정하지 않으면 모든 1 크기의 차원을 제거, 인덱스를 지정하면 특정 차원만 제거한다.\n",
    "- unsqueeze()는 새로운 차원을 추가하여 텐서의 크기를 확장한다.\n",
    "- flatten()은 텐서를 1차원 텐서로 평탄화하며, flatten(start_dim, end_dim)으로 원하는 구간만 펼칠 수 있다.\n",
    "- permute()는 텐서의 차원 순서를 재배열한다.\n",
    "- transpose()는 두 개의 차원 순서를 서로 바꾼다.\n",
    "- t()는 transpose()의 단축형으로, 2차원 텐서에만 사용 가능하며 0번째와 1번째 차원을 서로 바꾼다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2324c-64fa-4c8a-b179-4178abb3aee0",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- 메모리에서 연속적인(contiguous) 텐서에만 사용할 수 있는 함수와 아닌 함수가 있다는 사실"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6be410-96f3-4da5-9c90-6a5e483f79db",
   "metadata": {},
   "source": [
    "## l_tensor_concat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47064746-1110-45e6-8328-c798006cf353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf55a3-c3ab-4326-b1f6-a8711e439959",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.cat() 함수는 인자로 주어진 텐서들을 특정 차원을 기준으로 합친다.\n",
    "- torch.cat() 함수에서 지정된 차원을 제외한 모든 차원의 크기는 같아야 한다.\n",
    "- torch.cat() 함수에서 새로운 텐서의 크기는 병합된 차원의 크기가 합쳐진 결과이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b3419-2de1-4eb4-8f64-96347a97099d",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- torch.cat() 함수에서 지정된 차원을 제외한 모든 차원의 크기는 같아야 한다는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f3f33-f97e-4ad1-8975-99a0a4c172bc",
   "metadata": {},
   "source": [
    "## m_tensor_stacking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ed6f0e3-c0dd-4e66-9d53-d44f5eafe2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9371c-212c-414e-aeae-7a198c41e430",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.cat(tensors, dim)과 torch.stack(tensors, dim) 모두 텐서의 차원을 추가하는 함수이다.\n",
    "- torch.stack()은 새로운 차원을 생성해서 여러 텐서를 쌓고, 모든 입력 텐서는 동일한 크기(shape)여야 한다.\n",
    "- torch.cat()은 이미 존재하는 차원에서 이어붙인다. 지정된 차원을 제외한 나머지 모든 차원의 크기가 동일해야 한다.\n",
    "- stack(t1, t2,dim)≡cat(unsqueeze(t1,dim),unsqueeze(t2,dim),dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54ce11-7efc-49ee-9645-a3f337138c90",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- stack(t1, t2,dim)≡cat(unsqueeze(t1,dim),unsqueeze(t2,dim),dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa810fe-9d19-4daa-be54-e087b4ea46f1",
   "metadata": {},
   "source": [
    "## n_tensor_vstack_hstack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf23ea8f-2ea5-45c2-9daf-37460d194f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "--------------------------------------------------\n",
      "torch.Size([2, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "print(\"-\" * 50)\n",
    "t19 = torch.dstack([t16, t17])\n",
    "print(t19.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b84b5a-ccfa-4762-855a-5d0636e232e4",
   "metadata": {},
   "source": [
    "### 해석\n",
    "- torch.vstack()은 텐서를 수직으로 쌓는 함수로, 첫 번째 차원을 따라 torch.cat() 함수를 수행하는 것과 같다. 입력 텐서들은 첫 번째 차원을 제외한 나머지 모든 차원의 크기가 동일해야 한다.\n",
    "- torch.hstack()은 텐서를 수평으로 쌓는 함수이며, 이는 두 번째 차원을 따라 torch.cat() 함수를 수행하는 것과 같다. 단, 1D 텐서의 경우 예외적으로 dim=0을 따른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4aea41-3b73-4ab0-99f0-139f7ee9f939",
   "metadata": {},
   "source": [
    "### 코드 추가\n",
    "- 텐서를 깊이로 쌓는 방법을 알기 위해 torch.dstack()에 관한 코드를 추가했다.\n",
    "- 추가된 코드를 구분하기 위해 print(\"-\" * 50)을 추가했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ec672-9566-410d-949e-28dcb766ec97",
   "metadata": {},
   "source": [
    "### 취득한 기술적 사항/고찰 내용\n",
    "- vstack(), hstack(), dstack() 등 모두 torch.cat() 함수를 편리하게 사용하는 별칭 함수라는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d64aa2-ccdb-4ef0-9ee5-7c30a536e0a1",
   "metadata": {},
   "source": [
    "## 숙제 후기\n",
    "코드 블럭 단위가 아닌 코드 변수 단위로 공부를 시작하다보니 시간도 꽤 걸렸고, 다양한 방식으로 텐서를 다루려고 코드를 추가했다가 이후 추가한 내용과 비슷한 코드들이 등장해 코드 추가 범위에서 뺀 적도 많았다. 과제에서 생각보다 텐서를 다양하게 다루어서 코드 추가를 많이 할 필요가 없었고, 코드 블럭 단위로 공부하는 게 더 효율적이라는 점을 이후 깨닫게 되고 과제 중반부부터는 코드 블럭 단위로 공부를 시작하게 됐다. 공부하면서 가장 신경 쓰였던 점은 결과 출력값에 tensor와 torch로 구분되어 출력된다는 점이었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
